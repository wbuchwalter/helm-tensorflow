{{- $relname := .Release.Name -}}
{{- $chartname := .Chart.Name -}}
{{- $repo := .Values.image.repository -}}
{{- $tag := .Values.image.tag -}}
{{- $lr := .Values.hyperparams.learningrate -}}
{{- $epochs := .Values.hyperparams.epochs -}}
{{- $batchsize := .Values.hyperparams.batchsize -}}

apiVersion: v1
kind: ConfigMap
metadata:
  name: tf-cluster-spec
data:
  clusterspec: >
        {
        {{- range $type, $nb := .Values.clusterSpec }}
          {{ $type | quote }}: [
          {{ range $i, $e := until (int $nb) }}
            "{{ $type }}-{{$i}}:8080",
          {{ end }}
              ],
        {{- end }}
        }
---
{{- range $type, $nb := .Values.clusterSpec }}
{{- range $i, $e := until (int $nb) }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $type }}-{{ $i }}
spec:      
  template:
    metadata:
      labels:
        job: {{ $type }}
        task: t{{$i}}
    spec:
      restartPolicy: OnFailure
      containers:
      - name: {{ $chartname }}
{{ if eq $type "worker"}}
        image: "{{ $repo }}:{{ $tag }}"
{{ else }}
        image: "{{ $repo }}:{{ $tag }}-cpu"        
{{ end }}
        imagePullPolicy: Always        
        env:
        - name: LD_LIBRARY_PATH
          value: /usr/lib/nvidia:/usr/lib/x86_64-linux-gnu
        - name: CLUSTER_CONFIG
          valueFrom:
            configMapKeyRef:
              name: tf-cluster-spec
              key: clusterspec
        - name: POD_NAME
          value: {{ $type }}-{{$i}}
        command:
          - python
          - main.py
          - --logdir 
          - /data/logs/{{ $relname }}-{{ $type }}-{{$i}}
          - --lr
          - {{ $lr | quote }}
          - --epochs
          - {{ $epochs | quote }}
          - --batch_size
          - {{ $batchsize | quote }}
{{ if eq $type "worker"}}
        resources:
          requests:
            alpha.kubernetes.io/nvidia-gpu: 1 
          limits:
            alpha.kubernetes.io/nvidia-gpu: 1 
        volumeMounts:
        - mountPath: /usr/local/nvidia/bin
          name: bin
        - mountPath: /usr/lib/nvidia
          name: lib
        - mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.1
          name: libcuda
        - mountPath: /data
          name: azurefile
      volumes:
        - name: bin
          hostPath: 
            path: /usr/lib/nvidia-378/bin
        - name: lib
          hostPath: 
            path: /usr/lib/nvidia-378
        - name: libcuda
          hostPath:
            path: /usr/lib/x86_64-linux-gnu/libcuda.so.1 
        - name: azurefile
          azureFile:
              secretName: azure-secret
              shareName: tf-data
              readOnly: false
{{end}}
---
{{- end }}
{{- end }}
apiVersion: v1
kind: Service
metadata:
  labels:
    app: tensorboard
  name: tensorboard
spec:
  ports:
  - port: 80
    targetPort: 6006
  selector:
    app: tensorboard
  type: LoadBalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: tensorboard
  name: tensorboard
spec:
  template:
    metadata:
      labels:
        app: tensorboard
    spec:
      volumes:
      - name: azurefile
        azureFile:
            secretName: azure-secret
            shareName: tf-data
            readOnly: true      
      containers:
      - name: tensorboard
        command: ["/bin/sh", "-c"]
        args: ["tensorboard --logdir /data/logs --host 0.0.0.0"]
        image: tensorflow/tensorflow
        ports:
        - containerPort: 6006
        volumeMounts:
        - mountPath: /data
          name: azurefile  